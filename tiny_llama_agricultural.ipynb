{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuYH0/GqpoFDLvLG+CLG4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetechno157-create/agri_chatbot/blob/main/tiny_llama_agricultural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers sentence-transformers faiss-cpu numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vnNxR49mz-Wu",
        "outputId": "b533f5ab-fe00-4337-a40f-ff22bd02abc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n"
      ],
      "metadata": {
        "id": "Y2bIWZU96JQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "4ROcCoh1BvW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyLlamaRAGChatbot:\n",
        "    def __init__(self,\n",
        "                 model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                 embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 max_context_length=2048,\n",
        "                 chunk_size=500,\n",
        "                 chunk_overlap=50):\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.max_context_length = max_context_length\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.chat_history = []\n",
        "\n",
        "        print(\"Loading TinyLlama model...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "        self.chunk_metadata = []\n",
        "        print(\"RAG Chatbot initialized successfully!\")\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(self.tokenizer.encode(text))\n",
        "\n",
        "    def load_text_files(self, file_paths: List[str]) -> List[str]:\n",
        "        documents = []\n",
        "        for file_path in file_paths:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "                    documents.append(content)\n",
        "                    print(f\"Loaded {file_path}: {len(content)} characters\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file_path}: {e}\")\n",
        "        return documents\n",
        "\n",
        "    def chunk_text(self, text: str, source_file: str) -> List[Dict]:\n",
        "        import re\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_tokens = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "            sentence_tokens = self.count_tokens(sentence)\n",
        "            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:\n",
        "                chunks.append({\n",
        "                    'content': current_chunk.strip(),\n",
        "                    'source': source_file,\n",
        "                    'token_count': current_tokens\n",
        "                })\n",
        "                overlap_text = ' '.join(current_chunk.split()[-self.chunk_overlap:])\n",
        "                current_chunk = overlap_text + ' ' + sentence\n",
        "                current_tokens = self.count_tokens(current_chunk)\n",
        "            else:\n",
        "                current_chunk += ' ' + sentence\n",
        "                current_tokens += sentence_tokens\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'content': current_chunk.strip(),\n",
        "                'source': source_file,\n",
        "                'token_count': current_tokens\n",
        "            })\n",
        "        return chunks\n",
        "\n",
        "    def build_rag_database(self, file_paths: List[str]):\n",
        "        print(\"Building RAG database...\")\n",
        "        documents = self.load_text_files(file_paths)\n",
        "        all_chunks = []\n",
        "        for i, doc in enumerate(documents):\n",
        "            file_name = os.path.basename(file_paths[i])\n",
        "            chunks = self.chunk_text(doc, file_name)\n",
        "            all_chunks.extend(chunks)\n",
        "            print(f\"Created {len(chunks)} chunks from {file_name}\")\n",
        "        self.chunks = [chunk['content'] for chunk in all_chunks]\n",
        "        self.chunk_metadata = all_chunks\n",
        "        print(\"Generating embeddings...\")\n",
        "        embeddings = self.embedding_model.encode(self.chunks)\n",
        "        import faiss\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        print(f\"RAG database built with {len(self.chunks)} chunks\")\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        if self.index is None:\n",
        "            return []\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        import faiss\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                results.append({\n",
        "                    'content': self.chunks[idx],\n",
        "                    'source': self.chunk_metadata[idx]['source'],\n",
        "                    'score': float(scores[0][i]),\n",
        "                    'tokens': self.chunk_metadata[idx]['token_count']\n",
        "                })\n",
        "        return results\n",
        "\n",
        "    def format_chat_history(self, max_history_tokens: int = 10000) -> str:\n",
        "        if not self.chat_history:\n",
        "            return \"\"\n",
        "        history_text = \"\"\n",
        "        current_tokens = 0\n",
        "        for exchange in reversed(self.chat_history):\n",
        "            exchange_text = f\"User: {exchange['human']}\\nAssistant: {exchange['assistant']}\\n\"\n",
        "            exchange_tokens = self.count_tokens(exchange_text)\n",
        "            if current_tokens + exchange_tokens > max_history_tokens:\n",
        "                break\n",
        "            history_text = exchange_text + history_text\n",
        "            current_tokens += exchange_tokens\n",
        "        return history_text\n",
        "\n",
        "    def generate_response(self, user_input: str) -> str:\n",
        "        relevant_chunks = self.retrieve_relevant_chunks(user_input, top_k=3)\n",
        "        context = \"\"\n",
        "        total_context_tokens = 0\n",
        "        max_context_tokens = self.max_context_length // 2  # Reserve half for generation\n",
        "        for chunk in relevant_chunks:\n",
        "            chunk_text = f\"Source ({chunk['source']}): {chunk['content']}\\n\\n\"\n",
        "            chunk_tokens = self.count_tokens(chunk_text)\n",
        "            if total_context_tokens + chunk_tokens > max_context_tokens:\n",
        "                break\n",
        "            context += chunk_text\n",
        "            total_context_tokens += chunk_tokens\n",
        "        history = self.format_chat_history(max_history_tokens=500)\n",
        "\n",
        "        # AGRICULTURAL-SPECIFIC PROMPT\n",
        "\n",
        "        system_prompt = \"\"\"\n",
        "        - YOUR ROLE IS TO PROVIDE THE INFORMATION RELATED TO AGRICULTURE ONLY, EXCEPT THIS YOU DONT HAVE TO ANSWER ANY OTHER QUESTIONS.\n",
        "        -MUST NEED TO FOLLOW THE FOLLOWING INSTRUCTIONS:\n",
        "        You are an expert AI assistant specialized strictly in agriculture.\n",
        "        -ignore the qeatiions from the different domains such as  IT, political, human science ,many more, you just need to deliver the queries realted to agriculture department only.\n",
        "You are an expert assistant whose sole purpose is to provide information about agriculture and farming.\n",
        "ONLY answer questions related to agriculture, crops, soil, irrigation, fertilizers, seeds, agri-technology, pests, livestock, or related fields.\n",
        "If a user's question is NOT about agriculture or your knowledge base/retrieved context does NOT contain relevant agricultural information, politely respond:\n",
        "\"I'm sorry, but I can only answer questions about agriculture.\"\n",
        "\n",
        "Never attempt to answer non-agricultural questions, and do not generate or invent answers that are outside the agricultural domain under any circumstances.\n",
        "Always base your answers ONLY on the provided agricultural context or data.\n",
        "never helucinate while giving the output.\n",
        "-never terminate the output, always pass the output according to the tokens , which is 2048  only, so dont exceed the result beyond the token limits.\n",
        "\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"<|system|>\n",
        "{system_prompt}\n",
        "Agricultural knowledge base context (may be empty):\n",
        "{context or '[No relevant agricultural information found in the database.]'}\n",
        "Chat history (for reference only):\n",
        "{history}\n",
        "<|user|>\n",
        "{user_input}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_context_length-200)\n",
        "        device = next(self.model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=3038,\n",
        "                temperature=0.2,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        assistant_start = full_response.find(\"<|assistant|>\")\n",
        "        if assistant_start != -1:\n",
        "            response = full_response[assistant_start + len(\"<|assistant|>\"):].strip()\n",
        "        else:\n",
        "            response = full_response[len(prompt):].strip()\n",
        "        return response\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        response = self.generate_response(user_input)\n",
        "        self.chat_history.append({\n",
        "            'human': user_input,\n",
        "            'assistant': response\n",
        "        })\n",
        "        if len(self.chat_history) > 10:\n",
        "            self.chat_history = self.chat_history[-10:]\n",
        "        return response\n",
        "\n",
        "    def save_chat_history(self, filepath: str):\n",
        "        import json\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.chat_history, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    def load_chat_history(self, filepath: str):\n",
        "        import json\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                self.chat_history = json.load(f)\n",
        "            print(f\"Loaded chat history with {len(self.chat_history)} exchanges\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading chat history: {e}\")"
      ],
      "metadata": {
        "id": "L846yOY93xPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    chatbot = TinyLlamaRAGChatbot()\n",
        "    text_files = [\n",
        "        \"/content/agriculture_data3.txt\",\n",
        "        \"/content/agriculture_dataset.txt\",\n",
        "        \"/content/combined_2.txt\"\n",
        "        \"/content/combined_text.txt\"\n",
        "    ]\n",
        "    chatbot.build_rag_database(text_files)\n",
        "    print(\"\\n🤖 TinyLlama RAG Chatbot is ready for agriculture domain!\")\n",
        "    print(\"Type 'quit' to exit, 'save' to save chat history\")\n",
        "    print(\"-\" * 50)\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        elif user_input.lower() == 'save':\n",
        "            chatbot.save_chat_history(\"chat_history.json\")\n",
        "            print(\"Chat history saved!\")\n",
        "            continue\n",
        "        elif not user_input:\n",
        "            continue\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"\\nAssistant: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PiFfjKxpSIpc",
        "outputId": "56166652-ceaa-401a-e7be-fe5add67dd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyLlama model...\n",
            "Loading embedding model...\n",
            "RAG Chatbot initialized successfully!\n",
            "Building RAG database...\n",
            "Loaded /content/agriculture_data3.txt: 72166 characters\n",
            "Loaded /content/agriculture_dataset.txt: 1347210 characters\n",
            "Error loading /content/combined_2.txt/content/combined_text.txt: [Errno 20] Not a directory: '/content/combined_2.txt/content/combined_text.txt'\n",
            "Created 66 chunks from agriculture_data3.txt\n",
            "Created 1055 chunks from agriculture_dataset.txt\n",
            "Generating embeddings...\n",
            "RAG database built with 1121 chunks\n",
            "\n",
            "🤖 TinyLlama RAG Chatbot is ready for agriculture domain!\n",
            "Type 'quit' to exit, 'save' to save chat history\n",
            "--------------------------------------------------\n",
            "\n",
            "You: how to install the python in terminal ? \n",
            "\n",
            "Assistant: Sure, here's how to install Python in a terminal:\n",
            "\n",
            "1. Open a terminal window\n",
            "2. Run the following command to install Python:\n",
            "\n",
            "```\n",
            "$ sudo apt-get update\n",
            "$ sudo apt-get install python3-pip\n",
            "```\n",
            "\n",
            "3. Install the required packages for the AI assistant:\n",
            "\n",
            "```\n",
            "$ pip3 install -r requirements.txt\n",
            "```\n",
            "\n",
            "4. Once the installation is complete, you can run the AI assistant using the following command:\n",
            "\n",
            "```\n",
            "$ python3 ai_assistant.py\n",
            "```\n",
            "\n",
            "That's it! You should now be able to use the AI assistant in your terminal.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2191062510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2191062510.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTlJYNd2echx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Which crops are grown during the summer season in Asia?\n",
        "\n",
        "How can soil fertility be improved naturally for better crop yield?"
      ],
      "metadata": {
        "id": "95st7j7Aec5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}